# Hướng dẫn Tích hợp Amazon Marketing Stream với PostgreSQL (Phương pháp API Endpoint tiết kiệm)

## Mục tiêu

Tài liệu này hướng dẫn bạn cách thiết lập một pipeline dữ liệu **hybrid**, **chi phí cực thấp** để nhận, xử lý và ghi dữ liệu từ Amazon Marketing Stream vào cơ sở dữ liệu **PostgreSQL** đang chạy trên VPS Ubuntu 22.04.

> **Lưu ý về Lựa chọn Dataset:** Hướng dẫn này sử dụng đồng thời hai dataset: `sp-traffic` (impressions, clicks, cost) và `sp-conversion` (orders, sales) để có được một dashboard PPC toàn diện.

---

## Tổng quan Kiến trúc Hybrid (Chi phí thấp)

**Luồng Ghi dữ liệu (Data Ingestion):**
`Amazon Ads API` → `AWS Kinesis Data Firehose` → `AWS Lambda (process-and-forward-stream-enkezi)` → `Internet (HTTPS)` → `Backend API (trên VPS)` → `PostgreSQL (trên VPS)`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`↘ Amazon S3 (my-amazon-stream-data-backup-enkezi-1234)` (Optional Backup)

---

## Phân tích Chi phí (Cost Analysis)

-   **Kinesis Data Firehose:** Rất thấp, ~$0.029/GB.
-   **AWS Lambda:** Gói miễn phí lớn, chi phí gần như **$0/tháng**.
-   **VPS Ubuntu:** Chi phí không đổi.
-   **Amazon S3 (Optional):** Vài cent mỗi tháng để sao lưu.

**Tổng chi phí ước tính trên AWS: ~$0-2/tháng.**

---

## Yêu cầu

1.  **Tài khoản AWS & AWS CLI.**
2.  **VPS Ubuntu 22.04** với PostgreSQL và backend Node.js/Express đang chạy.
3.  **Một tên miền** trỏ đến IP của VPS.
4.  **Quyền Admin trên Amazon Ads.**

---

## Phần 1: Cấu hình VPS và Backend API

### Bước 1: Tạo và Cấp quyền cho Bảng Dữ liệu Stream
Bảng này sẽ lưu trữ dữ liệu thô nhận được từ Lambda. Chúng ta không chỉ tạo bảng mà còn phải **cấp quyền** cho người dùng của ứng dụng để có thể ghi dữ liệu vào đó. Đây là bước quan trọng để sửa lỗi "Failed to write data to database".

1.  **Đăng nhập vào VPS của bạn qua SSH.**
2.  **Kết nối vào PostgreSQL:** Chạy lệnh sau để kết nối trực tiếp vào database `amazon_data_enkezi` với quyền quản trị:
    ```bash
    sudo -u postgres psql -d amazon_data_enkezi
    ```
3.  **Chạy lệnh SQL tạo bảng và cấp quyền:** Sau khi vào được giao diện của `psql` (dấu nhắc sẽ đổi thành `amazon_data_enkezi=#`), hãy sao chép (copy) **CHÍNH XÁC** toàn bộ nội dung SQL bên dưới. **Đừng quên thay đổi `enkezi`** thành tên người dùng database mà bạn đã cấu hình trong file `backend/.env`.

    ```sql
    -- Bảng lưu trữ dữ liệu stream thô từ Amazon Marketing Stream
    CREATE TABLE IF NOT EXISTS raw_stream_events (
        id SERIAL PRIMARY KEY,
        event_type TEXT,
        event_data JSONB,
        received_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
    );

    -- Tạo chỉ mục để tăng tốc độ truy vấn
    CREATE INDEX IF NOT EXISTS idx_rse_received_at ON raw_stream_events (received_at DESC);
    CREATE INDEX IF NOT EXISTS idx_rse_event_type ON raw_stream_events (event_type);

    -- =========================================================================
    -- == BƯỚC QUAN TRỌNG: Cấp quyền cho user của ứng dụng                     ==
    -- == Thay thế 'enkezi' bằng DB_USER thực tế từ file .env của bạn.       ==
    -- =========================================================================
    GRANT SELECT, INSERT, UPDATE, DELETE ON TABLE raw_stream_events TO enkezi;
    GRANT USAGE, SELECT ON SEQUENCE raw_stream_events_id_seq TO enkezi;
    ```
    Sau khi dán và chỉnh sửa, nhấn Enter để thực thi các lệnh.

4.  **Thoát khỏi PostgreSQL:** Gõ lệnh `\q` và nhấn Enter để quay lại terminal bình thường.

### Bước 2: Cập nhật Backend để Nhận Dữ liệu Stream
Chúng ta sẽ cập nhật backend để có thể **nhận** dữ liệu từ Lambda và cung cấp dữ liệu cho frontend.

#### 2A. Cập nhật `backend/routes/stream.js`
File này là nơi định nghĩa các API endpoint liên quan đến stream. Nó đã được cập nhật để xử lý cả hai nhiệm vụ:
-   **`POST /api/stream-ingest`**: Đây là một endpoint bảo mật mới, chỉ nhận dữ liệu từ AWS Lambda (được xác thực bằng một API key bí mật). Nó nhận các sự kiện stream và ghi chúng vào bảng `raw_stream_events` trong PostgreSQL.
-   **`GET /api/stream/metrics`**: Endpoint này dùng để cung cấp các chỉ số tổng hợp cho frontend.

#### 2B. Xác minh `backend/server.js`
File `backend/server.js` là file "lắp ráp" toàn bộ ứng dụng. Để các endpoint trong `stream.js` có thể được truy cập từ bên ngoài, `server.js` phải "đăng ký" chúng.

**Giải thích:**
-   **`server.js`** tạo ra ứng dụng `app` chính.
-   **`stream.js`** tạo ra một `router` (bộ định tuyến) chứa các endpoint chuyên biệt.
-   Lệnh `app.use('/api', streamRoutes);` trong `server.js` đóng vai trò là "cầu nối", ra lệnh cho ứng dụng chính: "Tất cả các yêu cầu (request) có đường dẫn bắt đầu bằng `/api` hãy chuyển cho `streamRoutes` xử lý".

**Hành động:** Mở file `backend/server.js` và **đảm bảo** rằng các dòng sau đã tồn tại. Nếu chưa, hãy thêm chúng vào đúng vị trí.
```javascript
// Gần đầu file server.js, cùng với các import khác
import streamRoutes from './routes/stream.js';

// ... (code khác của bạn ở giữa) ...

// Gần cuối file, trước dòng app.listen()
// Dòng này đăng ký tất cả các routes từ stream.js vào ứng dụng chính
app.use('/api', streamRoutes);
```

#### 2C. Cập nhật `backend/.env`
- Mở file `backend/.env` và thêm một biến môi trường mới.
- **QUAN TRỌNG:** Tự tạo một chuỗi ký tự ngẫu nhiên, dài và phức tạp cho giá trị này (ví dụ: dùng một trình tạo mật khẩu). Đây là "chìa khóa bí mật" để Lambda có thể gửi dữ liệu đến backend của bạn một cách an toàn.
```dotenv
# Thêm dòng này vào cuối file backend/.env
STREAM_INGEST_SECRET_KEY=your_super_long_and_secret_random_string_here
```

#### 2D. Khởi động lại Backend
Nếu bạn đang dùng `pm2` trên VPS, hãy khởi động lại ứng dụng backend để áp dụng tất cả các thay đổi:
```sh
pm2 restart ppc-auto-backend-enkezi
```

---

## Phần 2: Thiết lập Pipeline trên AWS

### Bước 3: AWS - Tạo IAM Role cho Lambda
Role này chỉ cần quyền cơ bản để chạy và ghi log.
1.  Mở dịch vụ **IAM** -> **Roles** -> **"Create role"**.
2.  **Trusted entity type:** Chọn **AWS service**, Use case: **Lambda**.
3.  **Add permissions:** Tìm và thêm policy `AWSLambdaBasicExecutionRole`.
4.  **Role name:** `Lambda-StreamForwarder-Role-enkezi`.
5.  Nhấp **"Create role"**.

### Bước 4: AWS - Tạo Lambda Function
Function này sẽ nhận dữ liệu từ Firehose và gửi nó đến API endpoint trên VPS của bạn.
1.  Mở dịch vụ **Lambda** -> **"Create function"**.
2.  **Function name:** `process-and-forward-stream-enkezi`.
3.  **Runtime:** **Node.js 20.x**.
4.  **Permissions:** Chọn **"Use an existing role"** và chọn `Lambda-StreamForwarder-Role-enkezi`.
5.  Nhấp **"Create function"**.
6.  Trong tab **Code source**, dán toàn bộ nội dung từ file `lambda_deployment_package/process-and-forward-stream.mjs.txt` vào file `index.mjs`.
7.  Trong tab **Configuration** -> **Environment variables**, thêm 2 biến sau:
    -   Key: `INGEST_API_URL`, Value: `https://enkezi.tababyco.com/api/stream-ingest`.
    -   Key: `INGEST_API_KEY`, Value: Dán chuỗi bí mật bạn đã tạo ở **Bước 2C**.
8.  Trong **General configuration**, tăng **Timeout** lên `1 minute`.
9.  Nhấp **"Deploy Changes"**.

### Bước 5: AWS - Tạo S3 Bucket để Sao lưu (Tùy chọn nhưng khuyến nghị)

Bucket S3 này sẽ lưu trữ một bản sao của tất cả dữ liệu thô từ stream, rất hữu ích cho việc gỡ lỗi hoặc xử lý lại dữ liệu trong tương lai.

1.  Mở dịch vụ **Amazon S3**.
2.  Nhấp vào nút **"Create bucket"**.
3.  **Bucket name:** Nhập một tên duy nhất trên toàn cầu. Ví dụ: `my-amazon-stream-data-backup-enkezi-1234`.
    > **Lưu ý:** Tên bucket S3 là duy nhất trên toàn thế giới. Nếu tên này đã được sử dụng, bạn sẽ cần chọn một tên khác (ví dụ: thêm ngày tháng hoặc một số ngẫu nhiên vào cuối).
4.  **AWS Region:** Chọn Region phù hợp với bạn (ví dụ: `us-east-1`).
5.  Trong mục **Block Public Access settings for this bucket**, hãy đảm bảo ô **"Block all public access"** được tích. Đây là cài đặt mặc định và an toàn nhất.
6.  Để các cài đặt còn lại mặc định và nhấp **"Create bucket"**.

### Bước 6: AWS - Tạo IAM Role cho Firehose (Bước Sửa lỗi Quan trọng)

Đây là bước quan trọng để sửa lỗi `Firehose is unable to assume role`. Firehose cần một bộ quyền riêng để có thể gọi Lambda function của bạn và ghi dữ liệu vào S3 bucket.

1.  Mở dịch vụ **IAM** -> **Roles** -> **"Create role"**.
2.  **Trusted entity type:** Chọn **AWS service**.
3.  **Use case:** Tìm và chọn **Firehose** trong danh sách. Nhấp **Next**.
4.  Trang "Add permissions" sẽ hiện ra. Chính sách `AWSFirehoseServiceRolePolicy` thường được tự động đính kèm. Chúng ta cần thêm quyền gọi Lambda. Nhấp **Next**.
5.  **Role name:** `Firehose-Delivery-Role-enkezi`. Xem lại và nhấp **"Create role"**.
6.  **Thêm quyền gọi Lambda:**
    -   Từ danh sách Roles, tìm và nhấp vào role `Firehose-Delivery-Role-enkezi` bạn vừa tạo.
    -   Trong tab **Permissions**, nhấp vào **Add permissions** -> **Create inline policy**.
    -   Chọn tab **JSON** và dán nội dung sau vào:
        ```json
        {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Action": [
                        "lambda:InvokeFunction",
                        "lambda:GetFunctionConfiguration"
                    ],
                    "Resource": "arn:aws:lambda:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT_ID:function:process-and-forward-stream-enkezi"
                }
            ]
        }
        ```
    -   **QUAN TRỌNG:** Hãy thay thế `YOUR_AWS_REGION` (ví dụ: `us-east-1`) và `YOUR_AWS_ACCOUNT_ID` bằng thông tin tài khoản của bạn. Bạn có thể tìm thấy Account ID ở góc trên bên phải của AWS Console.
7.  Nhấp **Review policy**.
8.  **Name:** `Firehose-Invoke-Lambda-Policy-enkezi`.
9.  Nhấp **Create policy**.

### Bước 7: AWS - Cấu hình Firehose
Bây giờ chúng ta sẽ tạo Firehose stream và sử dụng Role vừa tạo.
1.  Mở **Kinesis** -> **Delivery streams** -> **"Create delivery stream"**.
2.  **Source:** `Direct PUT`.
3.  **Destination:** `Amazon S3`.
4.  **Delivery stream name:** `amazon-ads-firehose-stream-enkezi`.
5.  Trong mục **Data transformation**, chọn **Enabled**.
6.  **Lambda function:** Chọn `process-and-forward-stream-enkezi`.
7.  **Destination settings (S3 Backup):** Chọn S3 bucket bạn vừa tạo ở **Bước 5** (ví dụ: `my-amazon-stream-data-backup-enkezi-1234`).
8.  **Permissions:** Cuộn xuống phần "Permissions", chọn **"Choose an existing IAM role"** và chọn role `Firehose-Delivery-Role-enkezi` bạn đã tạo ở **Bước 6**.
9.  Nhấp **"Create delivery stream"** và sao chép **ARN** của stream để dùng ở bước sau.

---

## Phần 3: Đăng ký Stream và Hoàn tất

### Bước 8: AWS - Tạo IAM Roles cho Amazon Ads
Bước này cho phép tài khoản Amazon Ads gửi dữ liệu vào tài khoản AWS của bạn. *Bạn chỉ cần làm một lần.*
(Sử dụng các tên role sau để dễ phân biệt: `AmazonAds-Firehose-Subscription-Role-enkezi` và `AmazonAds-Firehose-Subscriber-Role-enkezi`)

1.  Làm theo hướng dẫn chi tiết của Amazon [tại đây](https://advertising.amazon.com/API/docs/en-us/amazon-marketing-stream-guides/create-iam-resources) để tạo 2 roles này.
2.  **Đảm bảo các cấu hình sau:**
    -   **Subscription Role** phải có quyền `firehose:PutRecord` trên ARN của Firehose bạn vừa tạo ở **Bước 7**.
    -   **Subscriber Role** phải có quyền `iam:PassRole` trên ARN của Subscription Role.
    -   **Trust policies** phải được cấu hình để tin tưởng Account ID của Amazon cho khu vực của bạn (NA: `926844853897`, EU: `668473351658`, FE: `074266271188`).
3.  Sau khi tạo, **sao chép ARN** của cả hai roles.

### Bước 9: Cập nhật `.env` và Đăng ký Stream

1.  Mở file `backend/.env`.
2.  Điền đầy đủ các giá trị sau:
    ```dotenv
    ADS_API_FIREHOSE_ARN=... (ARN từ Bước 7)
    ADS_API_FIREHOSE_SUBSCRIPTION_ROLE_ARN=... (ARN của Subscription Role từ Bước 8)
    ADS_API_FIREHOSE_SUBSCRIBER_ROLE_ARN=... (ARN của Subscriber Role từ Bước 8)
    ```
3.  Chạy lệnh sau từ thư mục gốc của dự án để đăng ký cả hai stream (`sp-traffic` và `sp-conversion`):
    ```sh
    npm run stream:subscribe
    ```

### Bước 10: Kiểm tra
Sau khi đăng ký thành công, dữ liệu sẽ bắt đầu chảy.
- **Trên AWS:** Kiểm tra CloudWatch logs của Lambda `process-and-forward-stream-enkezi` để xem các thông báo "Successfully forwarded".
- **Trên VPS:** Kiểm tra logs của ứng dụng backend (ví dụ `pm2 logs ppc-auto-backend-enkezi`) để xem các thông báo "Successfully ingested".
- **Trong Database:** Truy vấn bảng `raw_stream_events` để thấy dữ liệu mới được thêm vào.

Chúc mừng! Bạn đã thiết lập thành công pipeline để đưa dữ liệu Marketing Stream vào PostgreSQL trên VPS của mình một cách hiệu quả và tiết kiệm chi phí.