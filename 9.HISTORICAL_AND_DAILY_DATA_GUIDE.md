# Hướng Dẫn: Tải Dữ liệu Lịch sử & Tự động hóa Hàng ngày

## Giới thiệu

Chúc mừng bạn đã thiết lập thành công các script để lấy dữ liệu từ Amazon! Giờ là lúc đưa chúng vào hoạt động. Hướng dẫn này sẽ chỉ cho bạn 3 bước cốt lõi:

1.  **Chuẩn bị Database:** Chạy các file migration SQL để tạo các bảng cần thiết và **cấp quyền truy cập** cho ứng dụng.
2.  **Tải Dữ liệu Lịch sử (Backfilling):** Chạy các script một lần để lấy dữ liệu từ 60-90 ngày qua, cung cấp ngữ cảnh cho các phân tích của bạn.
3.  **Tự động hóa Hàng ngày:** Thiết lập một "cron job" trên server để tự động lấy dữ liệu của ngày hôm qua, mỗi ngày, giữ cho database của bạn luôn mới.

---

## Yêu cầu

-   Bạn đã đăng nhập vào VPS của mình qua SSH.
-   Toàn bộ code dự án đã có trên VPS.
-   File `backend/.env` đã được cấu hình đầy đủ và chính xác.

---

## Phần 1: Chuẩn bị Cơ sở dữ liệu (Chạy Migrations)

Trước khi có thể lưu dữ liệu, chúng ta cần tạo các bảng tương ứng trong PostgreSQL và cấp quyền cho user của ứng dụng.

> **QUAN TRỌNG:** Nếu bạn đã chạy bước này trước đây và gặp lỗi "permission denied", hãy chạy lại các lệnh bên dưới. Các file migration đã được cập nhật để sửa lỗi này.

1.  **Mở file migration và cập nhật Tên User:**
    -   **Đây là bước quan trọng để tránh lỗi "permission denied".**
    -   Mở file `backend/migrations/003_add_sp_search_term_report_table.sql.txt` và `backend/migrations/004_add_sales_and_traffic_tables.sql.txt` trên VPS của bạn (ví dụ: dùng lệnh `nano <path_to_file>`).
    -   Ở cuối mỗi file, bạn sẽ thấy một dòng `GRANT ... TO enkezi;`.
    -   **Hãy thay thế `enkezi`** bằng giá trị `DB_USER` thực tế từ file `backend/.env` của bạn. Lưu lại các file.

2.  **Đăng nhập vào PostgreSQL:**
    -   Sử dụng lệnh sau để kết nối trực tiếp vào database của bạn với quyền quản trị.
        ```bash
        sudo -u postgres psql -d amazon_data_enkezi
        ```
    -   Dấu nhắc lệnh sẽ đổi thành `amazon_data_enkezi=#`.

3.  **Chạy file Migration cho Search Term Report:**
    -   Sử dụng lệnh `\i` của `psql` để thực thi một file. Hãy thay `/var/www/Enkezi-Ads-Auto` bằng đường dẫn **CHÍNH XÁC** đến thư mục dự án của bạn.
        ```sql
        \i /var/www/Enkezi-Ads-Auto/backend/migrations/003_add_sp_search_term_report_table.sql.txt
        ```
    -   Bạn sẽ thấy các thông báo như `CREATE TABLE` và `GRANT`, cho biết lệnh đã thành công.

4.  **Chạy file Migration cho Sales & Traffic Report:**
    -   Tương tự, chạy lệnh cho file thứ hai:
        ```sql
        \i /var/www/Enkezi-Ads-Auto/backend/migrations/004_add_sales_and_traffic_tables.sql.txt
        ```

5.  **Thoát khỏi psql:**
    -   Gõ `\q` và nhấn Enter để quay lại terminal bình thường.

**Database của bạn giờ đã sẵn sàng để nhận dữ liệu!**

---

## Phần 2: Tải Dữ liệu Lịch sử (Backfilling)

Bây giờ chúng ta sẽ chạy các script để lấp đầy các bảng vừa tạo với dữ liệu từ 60 ngày qua. Việc tải dữ liệu lịch sử có thể mất nhiều thời gian, vì vậy bạn cần chạy chúng dưới dạng tiến trình nền (background process) để chúng không bị tắt khi bạn đóng cửa sổ dòng lệnh.

**Quan trọng:** Hãy chạy các lệnh này bên trong thư mục gốc của dự án của bạn (ví dụ: `/var/www/Enkezi-Ads-Auto`).

### 2.1. Di chuyển đến thư mục dự án
```bash
cd /var/www/Enkezi-Ads-Auto
```

### 2.2. Chọn phương pháp chạy nền

Có hai phương pháp phổ biến: `nohup` (đơn giản) và `screen` (linh hoạt và được khuyến nghị).

#### Phương pháp 1: Chạy ngầm với `nohup`

`nohup` cho phép một lệnh tiếp tục chạy ngay cả khi bạn đã đăng xuất.

1.  **Tải Dữ liệu Báo cáo Search Term:**
    ```bash
    # Ví dụ: Tải dữ liệu từ ngày 5 tháng 8 đến ngày 4 tháng 9 năm 2024
    nohup npm run fetch:sp-search-terms -- 2025-08-05 2025-09-04 &
    ```
2.  **Tải Dữ liệu Báo cáo Sales & Traffic:**
    ```bash
    # Ví dụ: Tải dữ liệu cho một khoảng thời gian dài hơn
    nohup npm run fetch:sales-traffic -- 2025-09-07 2025-09-07 &
    ```
3.  **Kiểm tra tiến trình:**
    -   `nohup` sẽ tạo một file `nohup.out` chứa log. Xem nó bằng lệnh: `tail -f nohup.out`
    -   Kiểm tra các tiến trình đang chạy: `ps aux | grep node`

#### Phương pháp 2: Chạy ngầm với `screen` (Phương pháp được khuyến nghị)

`screen` tạo ra các "phiên terminal ảo" mà bạn có thể ngắt kết nối và kết nối lại bất cứ lúc nào để xem tiến trình. Cách này rất tốt để chạy nhiều tác vụ song song và quản lý chúng một cách rõ ràng.

##### Tác vụ 1: Chạy `fetch:sp-search-terms`
1.  **Bắt đầu phiên đầu tiên và đặt tên cho nó:**
    ```bash
    screen -S sp-search-terms
    ```
2.  Một cửa sổ terminal mới sẽ hiện ra. Bên trong đó, chạy lệnh fetch:
    ```bash
    npm run fetch:sp-search-terms -- 2025-09-06 2025-09-06
    ```
3.  **Ngắt kết nối (Detach):** Nhấn tổ hợp phím **`Ctrl + A`**, sau đó nhấn phím **`d`**. Bạn sẽ quay lại terminal chính, nhưng script vẫn đang chạy.

##### Tác vụ 2: Chạy `fetch:sales-traffic`
1.  **Bắt đầu một phiên thứ hai** với tên khác:
    ```bash
    screen -S sales-traffic
    ```
2.  Trong phiên thứ hai này, chạy lệnh fetch còn lại:
    ```bash
    npm run fetch:sales-traffic -- 2025-09-07 2025-09-07
    ```
3.  **Ngắt kết nối (Detach):** Nhấn **`Ctrl + A`**, sau đó **`d`**.

##### Cách quản lý các phiên `screen`:
-   **Liệt kê tất cả các phiên đang chạy:**
    ```bash
    screen -ls
    ```
-   **Kết nối lại vào một phiên cụ thể để xem log:**
    ```bash
    # Kết nối lại vào phiên tải search terms
    screen -r sp-search-terms

    # Hoặc kết nối lại vào phiên tải sales & traffic
    screen -r sales-traffic
    ```

Hãy lặp lại quá trình này với các khoảng thời gian khác nhau cho đến khi bạn có đủ dữ liệu lịch sử mình cần.

---

## Phần 3: Tự động hóa Lấy Dữ liệu Hàng ngày với Cron

Đây là bước thiết lập một lần và hệ thống sẽ tự chạy mãi mãi. Chúng ta sẽ tạo một "cron job", một tác vụ được lập lịch trên Linux, để tự động chạy các script mỗi ngày một lần vào lúc 1 giờ sáng để lấy dữ liệu của 2 ngày trước, mỗi ngày, giữ cho database của bạn luôn mới. Dữ liệu quảng cáo thường có độ trễ, vì vậy lấy dữ liệu của 2 ngày trước sẽ đảm bảo dữ liệu đầy đủ và chính xác hơn.

1.  **Mở trình chỉnh sửa Crontab:**
    ```bash
    crontab -e
    ```
    -   Nếu đây là lần đầu tiên, bạn có thể được yêu cầu chọn một trình soạn thảo. Hãy chọn `nano` (thường là lựa chọn dễ nhất).

2.  **Thêm các dòng Cron Job:**
    -   Di chuyển con trỏ xuống cuối file và dán nội dung sau vào.
    -   **CỰC KỲ QUAN TRỌNG:** Hãy thay đổi `/var/www/Enkezi-Ads-Auto` thành đường dẫn **CHÍNH XÁC** đến thư mục dự án của bạn trên VPS.
    -   **Giải thích:**
        -   `TZ=America/Phoenix`: **Thiết lập Múi giờ.** Dòng này đặt múi giờ cho tất cả các tác vụ bên dưới thành **UTC-7** (giờ Phoenix, không đổi theo mùa). Tất cả thời gian sẽ được tính theo múi giờ này thay vì giờ hệ thống của server (thường là UTC).
        -   `0 1 * * *`: Lập lịch chạy vào **1:00 AM** (theo múi giờ đã đặt ở trên).
        -   `cd ...`: Di chuyển vào thư mục dự án trước khi chạy lệnh.
        -   `npm run ...`: Chạy script thông qua npm.
        -   `$(date ...)`: Tự động tạo ra ngày của 2 ngày trước theo định dạng YYYY-MM-DD.
        -   `>> ... 2>&1`: Ghi lại tất cả output (kể cả lỗi) vào một file log để bạn có thể kiểm tra sau này.
    -   **Lưu ý quan trọng về Môi trường Cron:** Đôi khi, môi trường mà cron chạy rất tối giản và không biết `node` hay `npm` ở đâu. Nếu cron job không chạy, bạn có thể cần thêm dòng `PATH` ở đầu file crontab của mình. Để tìm đường dẫn, chạy lệnh `which npm` trong terminal của bạn và copy kết quả.
        ```cron
        # Ví dụ: Thêm dòng này vào đầu file crontab nếu cần, BÊN DƯỚI dòng TZ
        # PATH=/home/yourusername/.nvm/versions/node/v22.17.1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
        ```

    ```cron
    # Đặt múi giờ cho tất cả các cron job là UTC-7 (America/Phoenix)
    TZ=America/Phoenix

    # Tự động lấy báo cáo Sponsored Products Search Term hàng ngày lúc 1:00 AM cho dữ liệu của 2 ngày trước
    0 1 * * * cd /var/www/Enkezi-Ads-Auto && npm run fetch:sp-search-terms -- $(date -d "2 days ago" +\%Y-\%m-\%d) $(date -d "2 days ago" +\%Y-\%m-\%d) >> /var/www/Enkezi-Ads-Auto/logs/fetch_sp_search_term.log 2>&1

    # Tự động lấy báo cáo Sales & Traffic hàng ngày lúc 1:05 AM cho dữ liệu của 2 ngày trước
    5 1 * * * cd /var/www/Enkezi-Ads-Auto && npm run fetch:sales-traffic -- $(date -d "2 days ago" +\%Y-\%m-\%d) $(date -d "2 days ago" +\%Y-\%m-\%d) >> /var/www/Enkezi-Ads-Auto/logs/fetch_sales_traffic.log 2>&1
    ```

3.  **Lưu và Thoát:**
    -   Nhấn `Ctrl + X`, sau đó `Y`, và `Enter` để lưu lại.
    -   Bạn sẽ thấy thông báo `crontab: installing new crontab`.

4.  **Tạo thư mục Logs:**
    -   Cron job sẽ ghi log vào thư mục `logs`, chúng ta cần tạo nó.
    -   Từ thư mục gốc của dự án, chạy lệnh:
        ```bash
        mkdir -p logs
        ```

## Hoàn tất!

Hệ thống của bạn giờ đã được thiết lập hoàn chỉnh! Nó không chỉ chứa dữ liệu lịch sử mà còn sẽ tự động cập nhật dữ liệu mới mỗi ngày. Ngày mai, bạn có thể kiểm tra các file trong thư mục `logs` để xác nhận rằng các cron job đã chạy thành công.

---

## Xử lý sự cố (Troubleshooting)

### Lỗi: `permission denied for table ...`

- **Nguyên nhân:** Lỗi này xảy ra khi người dùng database được định nghĩa trong `backend/.env` (ví dụ: `enkezi`) không có quyền đọc/ghi trên bảng mà script đang cố gắng truy cập. Bảng này được tạo bởi người dùng `postgres` và cần được cấp quyền một cách rõ ràng.
- **Giải pháp:**
    1. Quay lại **Phần 1** của hướng dẫn này.
    2. Đảm bảo rằng bạn đã **thay thế `enkezi`** trong các file migration `.sql.txt` bằng đúng tên `DB_USER` từ file `.env` của bạn.
    3. **Chạy lại các lệnh `\i`** trong `psql`. Thao tác này an toàn để chạy lại và sẽ áp dụng các quyền còn thiếu.